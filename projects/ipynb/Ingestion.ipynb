{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d0426-3929-44e5-bd47-87bd320c9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructField, StructType, IntegerType, StringType, BooleanType, DateType, DecimalType)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import trim, lower, initcap, regexp_replace, col, when, lit, to_date\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# ‚öôÔ∏è 1. Spark Session Initialization\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"[INFO] Initializing Spark session...\")\n",
    "\n",
    "custom_tmp_dir = \"E:/DataEngineering/SparkTemp\"  # Custom temp directory\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IPL Data Analysis SPARK\") \\\n",
    "    .config(\"spark.local.dir\", custom_tmp_dir) \\\n",
    "    .config(\"spark.files.overwrite\", \"false\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \n",
    "            \"E:/DataEngineering/keys/data-with-jha-0e1c0496e4ff.json\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \n",
    "            \"E:\\\\DataEngineering\\\\Ipl-Analytics\\\\jars\\\\gcs-connector-hadoop3-2.2.5-shaded.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"[INFO] Spark session initialized successfully.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# üìÑ 2. Function Definitions\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def clean_dataframe(df, key_columns=None,string_columns=None, integer_columns=None,date_columns=None, dedup_columns=None, table_name=\"Table\"):\n",
    "    print(f\"\\n Starting Cleaning for: {table_name}\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    initial_count = df.count()\n",
    "    print(f\"Initial Record Count: {initial_count}\")\n",
    "\n",
    "\n",
    "    if integer_columns:\n",
    "        print(\"Cleaning integer columns by replacing blanks/nulls with 0...\")\n",
    "        for col_name in integer_columns:\n",
    "            if col_name in df.columns:\n",
    "                df = df.withColumn(\n",
    "                    col_name,\n",
    "                    when(\n",
    "                        col(col_name).isNull() | (trim(col(col_name).cast(\"string\")) == \"\"),\n",
    "                        lit(0)\n",
    "                    ).otherwise(col(col_name).cast(\"int\"))\n",
    "                )\n",
    "\n",
    "        # Logging counts of replacements\n",
    "        # for col_name in integer_columns:\n",
    "        #     if col_name in df.columns:\n",
    "        #         count_nulls_or_blanks = df.filter(\n",
    "        #             col(col_name).isNull() | (trim(col(col_name).cast(\"string\")) == \"\")\n",
    "        #         ).count()\n",
    "        #         print(f\"Integer Column '{col_name}': {count_nulls_or_blanks} blanks/nulls replaced with 0\")\n",
    "\n",
    "\n",
    "\n",
    "    # from pyspark.sql.functions import lower\n",
    "    # if boolean_columns:\n",
    "    #     print(\"Cleaning boolean columns by replacing blanks/nulls with False (0)...\")\n",
    "    #     for col_name in boolean_columns:\n",
    "    #         if col_name in df.columns:\n",
    "    #             df = df.withColumn(\n",
    "    #                 col_name,\n",
    "    #                 when(col(col_name).isNull(), lit(False))\n",
    "    #                 .when(lower(col(col_name).cast(\"string\")).isin(\"true\", \"1\"), lit(True))\n",
    "    #                 .when(lower(col(col_name).cast(\"string\")).isin(\"false\", \"0\", \"\", \"null\"), lit(False))\n",
    "    #                 .otherwise(lit(False))  # default fallback\n",
    "    #             )\n",
    "\n",
    "    #     for col_name in boolean_columns:\n",
    "    #         if col_name in df.columns:\n",
    "    #             count_nulls = df.filter(\n",
    "    #                 col(col_name).isNull() | (lower(col(col_name).cast(\"string\")).isin(\"\", \"null\"))\n",
    "    #             ).count()\n",
    "    #             print(f\"Boolean Column '{col_name}': {count_nulls} blanks/nulls replaced with False (0)\")\n",
    "\n",
    "            \n",
    "\n",
    "    # Step X: Replace blank or null string values with \"BLANK\" and count replacements\n",
    "    print(\"Replacing NULL or empty string values in string columns with 'BLANK'...\")\n",
    "\n",
    "    for col_name in string_columns:\n",
    "        blank_condition = (col(col_name).isNull()) | (trim(col(col_name)) == \"\")\n",
    "        count_blank = df.filter(blank_condition).count()\n",
    "\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            when(blank_condition, lit(\"BLANK\")).otherwise(col(col_name))\n",
    "        )\n",
    "\n",
    "        # print(f\"Column '{col_name}': {count_blank} values replaced with 'BLANK'\")\n",
    "\n",
    "    print(\"String column 'BLANK' substitution complete.\")\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    # Step 1: Drop rows where all columns are null\n",
    "    df = df.dropna(how=\"all\")\n",
    "    after_null_drop = df.count()\n",
    "    print(f\" Step 1 - NULL row drop: {after_null_drop} | Removed: {initial_count - after_null_drop}\")\n",
    "\n",
    "    # Step 2: Filter based on essential key columns\n",
    "    if key_columns:\n",
    "        print(f\" Step 2 - Filtering nulls in key columns: {key_columns}\")\n",
    "        condition = None\n",
    "        for col_name in key_columns:\n",
    "            if condition is None:\n",
    "                condition = F.col(col_name).isNotNull()\n",
    "            else:\n",
    "                condition &= F.col(col_name).isNotNull()\n",
    "        df = df.filter(condition)\n",
    "    after_key_filter = df.count()\n",
    "    print(f\"Rows after key filters: {after_key_filter} | Removed: {after_null_drop - after_key_filter}\")\n",
    "\n",
    "    # Step 3: Standardize string columns\n",
    "    if string_columns:\n",
    "        print(f\"Step 3 - Cleaning string columns: {string_columns}\")\n",
    "        for col_name in string_columns:\n",
    "            df = df.withColumn(\n",
    "                col_name,\n",
    "                initcap(\n",
    "                    regexp_replace(trim(lower(F.col(col_name))), \" +\", \" \")\n",
    "                )\n",
    "            )\n",
    "    after_string_clean = df.count()\n",
    "    print(f\"String columns cleaned. Record count: {after_string_clean}\")\n",
    "\n",
    "   # Step 4: Convert date columns\n",
    "    if date_columns:\n",
    "        print(f\"Step 4 - Formatting date columns: {date_columns}\")\n",
    "        for col_name in date_columns:\n",
    "             df = df.withColumn(\n",
    "    f\"{col_name}_cleaned\",\n",
    "    when(\n",
    "        col(f\"{col_name}\").rlike(r\"^\\d{1,2}/\\d{1,2}/\\d{4}$\"),\n",
    "        to_date(col(f\"{col_name}\"), \"M/d/yyyy\")\n",
    "    ).when(\n",
    "        col(f\"{col_name}\").rlike(r\"^\\d{2}-\\d{2}-\\d{4}$\"),\n",
    "        to_date(col(f\"{col_name}\"), \"dd-MM-yyyy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "    after_date_conversion = df.count()\n",
    "    print(f\"Date formatting done. Record count: {after_date_conversion}\")\n",
    "\n",
    "    df = df.withColumn(f\"{col_name}\", trim(col(col_name).cast(\"string\")))\n",
    "\n",
    "    \n",
    "     #Step 5: Deduplication\n",
    "    if dedup_columns:\n",
    "        print(f\"Step 5 - Removing duplicates using: {dedup_columns}\")\n",
    "        before_dedup = df.count()\n",
    "        df = df.dropDuplicates(dedup_columns)\n",
    "        after_dedup = df.count()\n",
    "        print(f\"After deduplication: {after_dedup} | Duplicates removed: {before_dedup - after_dedup}\")\n",
    "    else:\n",
    "         after_dedup = after_date_conversion\n",
    "     # Final Summary\n",
    "    print(\"Final Cleaning Summary:\")\n",
    "    print(f\"Initial Records           : {initial_count}\")\n",
    "    print(f\"After NULL Row Drop       : {after_null_drop}\")\n",
    "    print(f\"After Key Filter          : {after_key_filter}\")\n",
    "    print(f\"After String Clean        : {after_string_clean}\")\n",
    "    print(f\"After Date Conversion     : {after_date_conversion}\")\n",
    "    print(f\"After deduplication       : {after_dedup}\")\n",
    "    print(f\"Final Cleaned Record Count: {after_dedup}\")\n",
    "    print(\"Cleaning Complete!\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def map_team_names(df, column, mapping_dict):\n",
    "    expr = None\n",
    "    for code, name in mapping_dict.items():\n",
    "        condition = (col(column) == code)\n",
    "        expr = when(condition, name) if expr is None else expr.when(condition, name)\n",
    "    expr = expr.otherwise(col(column))\n",
    "    return df.withColumn(column, expr)\n",
    "\n",
    "def parse_date_column(df, column):\n",
    "    return df.withColumn(\n",
    "        column,\n",
    "        when(col(column).rlike(r\"^\\d{1,2}/\\d{1,2}/\\d{4}$\"), to_date(col(column), \"MM/dd/yyyy\"))  # e.g., 4/22/2013\n",
    "        .when(col(column).rlike(r\"^\\d{2}-\\d{2}-\\d{4}$\"), to_date(col(column), \"MM/dd/yyyy\"))     # e.g., 05-12-2013\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# üìÑ 2. Schema Definitions\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"[INFO] Defining custom schemas for all input datasets...\")\n",
    "\n",
    "ball_by_ball_schema = StructType([\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"over_id\", IntegerType(), True),\n",
    "    StructField(\"ball_id\", IntegerType(), True),\n",
    "    StructField(\"innings_no\", IntegerType(), True),\n",
    "    StructField(\"team_batting\", StringType(), True),\n",
    "    StructField(\"team_bowling\", StringType(), True),\n",
    "    StructField(\"striker_batting_position\", IntegerType(), True),\n",
    "    StructField(\"extra_type\", StringType(), True),\n",
    "    StructField(\"runs_scored\", IntegerType(), True),\n",
    "    StructField(\"extra_runs\", IntegerType(), True),\n",
    "    StructField(\"wides\", IntegerType(), True),\n",
    "    StructField(\"legbyes\", IntegerType(), True),\n",
    "    StructField(\"byes\", IntegerType(), True),\n",
    "    StructField(\"noballs\", IntegerType(), True),\n",
    "    StructField(\"penalty\", IntegerType(), True),\n",
    "    StructField(\"bowler_extras\", IntegerType(), True),\n",
    "    StructField(\"out_type\", StringType(), True),\n",
    "    StructField(\"caught\", IntegerType(), True),\n",
    "    StructField(\"bowled\", IntegerType(), True),\n",
    "    StructField(\"run_out\", IntegerType(), True),\n",
    "    StructField(\"lbw\", IntegerType(), True),\n",
    "    StructField(\"retired_hurt\", IntegerType(), True),\n",
    "    StructField(\"stumped\", IntegerType(), True),\n",
    "    StructField(\"caught_and_bowled\", IntegerType(), True),\n",
    "    StructField(\"hit_wicket\", IntegerType(), True),\n",
    "    StructField(\"obstructingfeild\", IntegerType(), True),\n",
    "    StructField(\"bowler_wicket\", IntegerType(), True),\n",
    "    StructField(\"match_date\", StringType(), True),\n",
    "    StructField(\"season\", IntegerType(), True),\n",
    "    StructField(\"striker\", IntegerType(), True),\n",
    "    StructField(\"non_striker\", IntegerType(), True),\n",
    "    StructField(\"bowler\", IntegerType(), True),\n",
    "    StructField(\"player_out\", IntegerType(), True),\n",
    "    StructField(\"fielders\", IntegerType(), True),\n",
    "    StructField(\"striker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"strikersk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_match_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_match_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_sk\", IntegerType(), True),\n",
    "    StructField(\"playerout_match_sk\", IntegerType(), True),\n",
    "    StructField(\"battingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"bowlingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"keeper_catch\", IntegerType(), True),\n",
    "    StructField(\"player_out_sk\", IntegerType(), True),\n",
    "    StructField(\"matchdatesk\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "match_schema = StructType([\n",
    "    StructField(\"match_sk\", IntegerType(), True),\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"team1\", StringType(), True),\n",
    "    StructField(\"team2\", StringType(), True),\n",
    "    StructField(\"match_date\", StringType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),  # Year as IntegerType\n",
    "    StructField(\"venue_name\", StringType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"toss_winner\", StringType(), True),\n",
    "    StructField(\"match_winner\", StringType(), True),\n",
    "    StructField(\"toss_name\", StringType(), True),\n",
    "    StructField(\"win_type\", StringType(), True),\n",
    "    StructField(\"outcome_type\", StringType(), True),\n",
    "    StructField(\"manofmach\", StringType(), True),\n",
    "    StructField(\"win_margin\", IntegerType(), True),\n",
    "    StructField(\"country_id\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "player_schema = StructType([\n",
    "    StructField(\"player_sk\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "])\n",
    "\n",
    "player_match_schema = StructType([\n",
    "    StructField(\"player_match_sk\", IntegerType(), True),\n",
    "    StructField(\"playermatch_key\",StringType(), True),  # Adjust precision/scale as needed\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"role_desc\", StringType(), True),\n",
    "    StructField(\"player_team\", StringType(), True),\n",
    "    StructField(\"opposit_team\", StringType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),\n",
    "    StructField(\"is_manofthematch\", IntegerType(), True),\n",
    "    StructField(\"age_as_on_match\", IntegerType(), True),\n",
    "    StructField(\"isplayers_team_won\", IntegerType(), True),\n",
    "    StructField(\"batting_status\", StringType(), True),\n",
    "    StructField(\"bowling_status\", StringType(), True),\n",
    "    StructField(\"player_captain\", StringType(), True),\n",
    "    StructField(\"opposit_captain\", StringType(), True),\n",
    "    StructField(\"player_keeper\", StringType(), True),\n",
    "    StructField(\"opposit_keeper\", StringType(), True),\n",
    "])\n",
    "\n",
    "team_schema = StructType([\n",
    "    StructField(\"team_sk\", IntegerType(), True),\n",
    "    StructField(\"team_id\", IntegerType(), True),\n",
    "    StructField(\"team_name\", StringType(), True),\n",
    "])\n",
    "print(\"[INFO] Schema definitions completed.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# üì• 3. Load DataFrames from GCS\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"[INFO] Starting to load datasets from GCS...\")\n",
    "\n",
    "try:\n",
    "    print(\"[INFO] Loading Ball_By_Ball.csv...\")\n",
    "    df_ball_by_ball = spark.read.schema(ball_by_ball_schema).option(\"header\", \"true\").csv(\"gs://ipl-data-project/Ball_By_Ball.csv\")\n",
    "    print(\"[SUCCESS] Ball_By_Ball.csv loaded. Row count:\", df_ball_by_ball.count())\n",
    "\n",
    "    print(\"[INFO] Loading Match.csv...\")\n",
    "    df_match = spark.read.schema(match_schema).option(\"header\", \"true\").csv(\"gs://ipl-data-project/Match.csv\")\n",
    "    print(\"[SUCCESS] Match.csv loaded. Row count:\", df_match.count())\n",
    "\n",
    "    print(\"[INFO] Loading Player.csv...\")\n",
    "    df_player = spark.read.schema(player_schema).option(\"header\", \"true\").csv(\"gs://ipl-data-project/Player.csv\")\n",
    "    print(\"[SUCCESS] Player.csv loaded. Row count:\", df_player.count())\n",
    "\n",
    "    print(\"[INFO] Loading Player_match.csv...\")\n",
    "    df_player_match = spark.read.schema(player_match_schema).option(\"header\", \"true\").csv(\"gs://ipl-data-project/Player_match.csv\")\n",
    "    print(\"[SUCCESS] Player_match.csv loaded. Row count:\", df_player_match.count())\n",
    "\n",
    "    print(\"[INFO] Loading Team.csv...\")\n",
    "    df_team = spark.read.schema(team_schema).option(\"header\", \"true\").csv(\"gs://ipl-data-project/Team.csv\")\n",
    "    print(\"[SUCCESS] Team.csv loaded. Row count:\", df_team.count())\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Failed during data loading:\", e)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# üßæ 4. Schema Validation\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[INFO] Printing all schemas for verification...\")\n",
    "\n",
    "df_ball_by_ball.printSchema()\n",
    "df_match.printSchema()\n",
    "df_player.printSchema()\n",
    "df_player_match.printSchema()\n",
    "df_team.printSchema()\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# ‚úÖ Final Step\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[INFO] All datasets loaded and schemas verified.\")\n",
    "print(\"[COMPLETED] IPL Data Analysis environment is ready.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# DATA CLEANING STARTED\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Filtering nulls in key columns\n",
    "#Standardize string columns\n",
    "#Convert date columns\n",
    "print(\"--------------------------------*********************************CLEANING DATASET STARTED*********************************------------------------------------------------\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\" Ball by Ball Data Cleaning Started...\")\n",
    "team_mapping = {\n",
    "    \"1\": \"Kolkata Knight Riders\",\n",
    "    \"2\": \"Royal Challengers Bangalore\",\n",
    "    \"3\": \"Chennai Super Kings\",\n",
    "    \"4\": \"Kings XI Punjab\",\n",
    "    \"5\": \"Rajasthan Royals\",\n",
    "    \"6\": \"Delhi Daredevils\",\n",
    "    \"7\": \"Mumbai Indians\",\n",
    "    \"8\": \"Deccan Chargers\",\n",
    "    \"9\": \"Kochi Tuskers Kerala\",\n",
    "    \"10\": \"Pune Warriors\",\n",
    "    \"11\": \"Sunrisers Hyderabad\",\n",
    "    \"12\": \"Rising Pune Supergiants\",\n",
    "    \"13\": \"Gujarat Lions\"\n",
    "}\n",
    "\n",
    "# Step X: Map numeric codes in team columns to actual team names\n",
    "print(\" Mapping numeric team codes to full names in 'team_batting' and 'team_bowling'...\")\n",
    "for col_name in [\"team_batting\", \"team_bowling\"]:\n",
    "    df_ball_by_ball = map_team_names(df_ball_by_ball, col_name, team_mapping)\n",
    "\n",
    "print(\"Team mapping applied.\")\n",
    "print(\"See Results:\")\n",
    "df_ball_by_ball.select(\"team_batting\",\"team_bowling\").show(10)\n",
    "\n",
    "\n",
    "df_ball_by_ball_cleaned  = clean_dataframe(\n",
    "    df_ball_by_ball,\n",
    "    key_columns=[\"match_id\", \"over_id\", \"ball_id\"],\n",
    "    string_columns=[\"team_batting\", \"team_bowling\", \"extra_type\", \"out_type\"],\n",
    "    # boolean_columns=\n",
    "    # [\n",
    "    #     \"caught\", \"bowled\", \"run_out\", \"lbw\", \"retired_hurt\",\n",
    "    #     \"stumped\", \"caught_and_bowled\", \"hit_wicket\", \"obstructingfeild\", \"bowler_wicket\",\"keeper_catch\"\n",
    "    # ],\n",
    "    integer_columns=[\n",
    "        'striker_batting_position', 'runs_scored', 'extra_runs', 'wides',\n",
    "        'legbyes', 'byes', 'noballs', 'penalty', 'bowler_extras',\n",
    "        'striker', 'non_striker', 'bowler', 'player_out', 'fielders'\n",
    "    ],\n",
    "    date_columns=[\"match_date\"],\n",
    "    dedup_columns=[\"match_id\", \"over_id\", \"ball_id\"],\n",
    "    table_name=\"Ball_By_Ball\"\n",
    ")\n",
    "\n",
    "print(f\"Writing df_ball_by_ball_cleaned data to: output_dir\")\n",
    "df_ball_by_ball_cleaned.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv('E:/DataEngineering/Ipl-Analytics/cleaned-data/Ball_By_Ball')\n",
    "\n",
    "print(\"Write complete!\")\n",
    "print(\" Ball by Ball Data Cleaning ENDED...\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\" Match Data Cleaning Started...\")\n",
    "\n",
    "df_match_cleaned = clean_dataframe(\n",
    "    df_match,\n",
    "    key_columns=[\"match_id\"],\n",
    "    string_columns=[\"team1\", \"team2\", \"venue_name\", \"city_name\", \"country_name\", \"toss_winner\", \"match_winner\", \"toss_name\", \"win_type\", \"outcome_type\", \"manofmach\"],\n",
    "    date_columns=[\"match_date\"],\n",
    "    dedup_columns=[\"match_id\"],\n",
    "    table_name=\"Match\"\n",
    ")\n",
    "print(f\"Writing df_match_cleaned data to: output_dir\")\n",
    "df_match_cleaned.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv('E:/DataEngineering/Ipl-Analytics/cleaned-data/Match')\n",
    "\n",
    "print(\" Match Data Cleaning ENDED...\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\" Player Data Cleaning Started...\")\n",
    "df_player_cleaned = clean_dataframe(\n",
    "    df_player,\n",
    "    key_columns=[\"player_id\"],\n",
    "    string_columns=[\"player_name\", \"batting_hand\", \"bowling_skill\", \"country_name\"],\n",
    "    date_columns=[\"dob\"],\n",
    "    dedup_columns=[\"player_id\"],\n",
    "    table_name=\"Player\"\n",
    ")\n",
    "print(f\"Writing df_player_cleaned data to: output_dir\")\n",
    "df_player_cleaned.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv('E:/DataEngineering/Ipl-Analytics/cleaned-data/Player')\n",
    "\n",
    "print(\" Player Data Cleaning ENDED...\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\" Player Match Data Cleaning Started...\")\n",
    "df_player_match_cleaned = clean_dataframe(\n",
    "    df_player_match,\n",
    "    key_columns=[\"player_match_sk\",\"playermatch_key\"],\n",
    "    string_columns=[\"player_name\", \"batting_hand\", \"bowling_skill\", \"country_name\", \"role_desc\", \"player_team\", \"opposit_team\", \"batting_status\", \"bowling_status\", \"player_captain\", \"opposit_captain\", \"player_keeper\", \"opposit_keeper\"],\n",
    "    date_columns=[\"dob\"],\n",
    "    dedup_columns=[\"player_match_sk\",\"playermatch_key\"],\n",
    "    table_name=\"Player_Match\"\n",
    ")\n",
    "print(f\"Writing df_player_match_cleaned data to: output_dir\")\n",
    "df_player_match_cleaned.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv('E:/DataEngineering/Ipl-Analytics/cleaned-data/PlayerMatch')\n",
    "\n",
    "print(\" Player Match Data Cleaning ENDED...\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\" Team Data Cleaning Started...\")\n",
    "df_team_cleaned = clean_dataframe(\n",
    "    df_team,\n",
    "    key_columns=[\"team_id\"],\n",
    "    string_columns=[\"team_name\"],\n",
    "    dedup_columns=[\"team_id\"],\n",
    "    table_name=\"Team\"\n",
    ")\n",
    "print(f\"Writing df_team_cleaned data to: output_dir\")\n",
    "df_team_cleaned.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv('E:/DataEngineering/Ipl-Analytics/cleaned-data/Team')\n",
    "\n",
    "print(\" Team Data Cleaning ENDED...\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"--------------------------------*********************************CLEANING DATASET ENDED*********************************------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
